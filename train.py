import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm 
import warnings

warnings.filterwarnings("ignore")

# å¯¼å…¥ä¹‹å‰å†™å¥½çš„æ¨¡å—
from DataSet import ForgeryDataset, TRAIN_TRANSFORM
from unet_model import UNet

# ==========================================
# 1. é…ç½®å‚æ•°
# ==========================================
DATA_ROOT = 'D:/InfSec/Data/recodai-luc-scientific-image-forgery-detection'  
TRAIN_IMG_DIR = os.path.join(DATA_ROOT, 'train_images')
TRAIN_MASK_DIR = os.path.join(DATA_ROOT, 'train_masks')

BATCH_SIZE = 4       # æ‰¹æ¬¡å¤§å° (æ˜¾å­˜å¦‚æœä¸å¤Ÿå¯ä»¥æ”¹å°åˆ° 2)
LEARNING_RATE = 1e-4 # å­¦ä¹ ç‡
NUM_EPOCHS = 5       # è®­ç»ƒè½®æ•° (å…ˆè¯•è·‘ 5 è½®çœ‹çœ‹æ•ˆæœ)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"æ­£åœ¨ä½¿ç”¨è®¾å¤‡: {DEVICE}")

def get_all_image_paths(root_dir):
    paths = []
    # æ‰«æ authentic å’Œ forged æ–‡ä»¶å¤¹
    for sub in ['authentic', 'forged']:
        sub_dir = os.path.join(root_dir, sub)
        if os.path.exists(sub_dir):
            for f in os.listdir(sub_dir):
                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.tif')):
                    paths.append(os.path.join(sub_dir, f))
    return paths

# ==========================================
# 2. è®­ç»ƒå¾ªç¯å‡½æ•°
# ==========================================
def train_fn(loader, model, optimizer, loss_fn, scaler):
    # ä¼˜åŒ– tqdm:
    # ncols=100: å›ºå®šå®½åº¦ï¼Œé˜²æ­¢æ¢è¡Œ
    # desc="Training": ç»™è¿›åº¦æ¡å·¦è¾¹åŠ ä¸ªæ ‡é¢˜
    loop = tqdm(loader, leave=True, ncols=100, desc="Training") 
    total_loss = 0
    
    for batch_idx, (data, targets, _) in enumerate(loop):
        data = data.to(DEVICE)
        targets = targets.to(DEVICE)

        # æ··åˆç²¾åº¦è®­ç»ƒ
        with torch.amp.autocast('cuda', enabled=(DEVICE=="cuda")): # ä¿®å¤è­¦å‘Šï¼šæ”¹ç”¨ torch.amp
            predictions = model(data)
            loss = loss_fn(predictions, targets)

        optimizer.zero_grad()
        if DEVICE == "cuda":
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            optimizer.step()

        # æ›´æ–°ç»Ÿè®¡
        total_loss += loss.item()
        
        # ä¼˜åŒ–æ˜¾ç¤º: åªåœ¨å³è¾¹æ˜¾ç¤º lossï¼Œä¿ç•™ 4 ä½å°æ•°
        loop.set_postfix(loss=f"{loss.item():.4f}")
        
    return total_loss / len(loader)

# ==========================================
# 3. ä¸»ç¨‹åº
# ==========================================
if __name__ == '__main__':
    # --- å‡†å¤‡æ•°æ® ---
    all_paths = get_all_image_paths(TRAIN_IMG_DIR)
    print(f"æ‰¾åˆ° {len(all_paths)} å¼ å›¾ç‰‡ç”¨äºè®­ç»ƒã€‚")
    
    if len(all_paths) == 0:
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°å›¾ç‰‡ï¼Œè¯·æ£€æŸ¥è·¯å¾„è®¾ç½®ï¼")
        exit()

    # åˆ›å»º Dataset å’Œ DataLoader
    # ä¸ºäº†æ¼”ç¤ºå¿«é€Ÿå¼€å§‹ï¼Œæš‚æ—¶ç”¨å…¨éƒ¨æ•°æ® (å¦‚æœå¤ªæ…¢ï¼Œå¯ä»¥ all_paths[:100] å…ˆæµ‹è¯•)
    ds = ForgeryDataset(all_paths, TRAIN_MASK_DIR, transform=TRAIN_TRANSFORM)
    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)

    # --- åˆå§‹åŒ–æ¨¡å‹ ---
    # n_channels=3 (RGB), n_classes=1 (Binary Mask)
    model = UNet(n_channels=3, n_classes=1).to(DEVICE)
    
    # å®šä¹‰æŸå¤±å‡½æ•° (BCEWithLogitsLoss ç»“åˆäº† Sigmoid å’Œ BCEï¼Œæ•°å€¼æ›´ç¨³å®š)
    loss_fn = nn.BCEWithLogitsLoss()
    
    # å®šä¹‰ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    # æ¢¯åº¦ç¼©æ”¾å™¨ (ç”¨äºæ··åˆç²¾åº¦è®­ç»ƒ)
    scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE=="cuda"))

    # --- å¼€å§‹è®­ç»ƒ ---
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    for epoch in range(NUM_EPOCHS):
        # æŠŠ Epoch ä¿¡æ¯æ‰“å°åœ¨è¿›åº¦æ¡ä¸Šæ–¹ï¼Œè€Œä¸æ˜¯æŒ¤åœ¨è¿›åº¦æ¡é‡Œ
        print(f"Epoch [{epoch+1}/{NUM_EPOCHS}]")
        
        avg_loss = train_fn(loader, model, optimizer, loss_fn, scaler)
        
        # æ‰“å°æœ¬è½®æ€»ç»“
        print(f"--> Average Loss: {avg_loss:.4f}")
        print(f"-------------------------------------------------------")
        
        # ä¿å­˜æ¨¡å‹
        checkpoint = {
            "state_dict": model.state_dict(),
            "optimizer": optimizer.state_dict(),
        }
        # åªä¿å­˜æœ€æ–°çš„ï¼Œæˆ–è€…æŒ‰ epoch å‘½å
        torch.save(checkpoint, f"checkpoint_epoch_{epoch+1}.pth.tar")


    print("\nğŸ‰ è®­ç»ƒå…¨éƒ¨å®Œæˆï¼")
